[32m[1203 23:15:20 @logger.py:73][0m Argv: /home/vigneshv/sfuhome/vigneshv/Computer_Vision_Ass_6/project6_package/code/run.py --task 2 --gpu 0
[32m[1203 23:16:02 @parallel.py:190][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[1203 23:16:02 @argtools.py:146][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[1203 23:16:53 @develop.py:99][0m [5m[31mWRN[0m [Deprecated] ModelDescBase._get_inputs() interface will be deprecated after 30 Mar. Use inputs() instead!
[32m[1203 23:16:53 @trainers.py:52][0m Building graph for a single training tower ...
[32m[1203 23:16:53 @develop.py:99][0m [5m[31mWRN[0m [Deprecated] ModelDescBase._build_graph() interface will be deprecated after 30 Mar. Use build_graph() instead!
[32m[1203 23:16:53 @registry.py:121][0m conv1_1 input: [None, 224, 224, 3]
[32m[1203 23:16:53 @registry.py:129][0m conv1_1 output: [None, 224, 224, 64]
[32m[1203 23:16:53 @registry.py:121][0m conv1_2 input: [None, 224, 224, 64]
[32m[1203 23:16:53 @registry.py:129][0m conv1_2 output: [None, 224, 224, 64]
[32m[1203 23:16:53 @registry.py:121][0m pool1 input: [None, 224, 224, 64]
[32m[1203 23:16:53 @registry.py:129][0m pool1 output: [None, 112, 112, 64]
[32m[1203 23:16:53 @registry.py:121][0m conv2_1 input: [None, 112, 112, 64]
[32m[1203 23:16:53 @registry.py:129][0m conv2_1 output: [None, 112, 112, 128]
[32m[1203 23:16:53 @registry.py:121][0m conv2_2 input: [None, 112, 112, 128]
[32m[1203 23:16:53 @registry.py:129][0m conv2_2 output: [None, 112, 112, 128]
[32m[1203 23:16:53 @registry.py:121][0m pool2 input: [None, 112, 112, 128]
[32m[1203 23:16:53 @registry.py:129][0m pool2 output: [None, 56, 56, 128]
[32m[1203 23:16:53 @registry.py:121][0m conv3_1 input: [None, 56, 56, 128]
[32m[1203 23:16:53 @registry.py:129][0m conv3_1 output: [None, 56, 56, 256]
[32m[1203 23:16:53 @registry.py:121][0m conv3_2 input: [None, 56, 56, 256]
[32m[1203 23:16:53 @registry.py:129][0m conv3_2 output: [None, 56, 56, 256]
[32m[1203 23:16:53 @registry.py:121][0m conv3_3 input: [None, 56, 56, 256]
[32m[1203 23:16:53 @registry.py:129][0m conv3_3 output: [None, 56, 56, 256]
[32m[1203 23:16:53 @registry.py:121][0m pool3 input: [None, 56, 56, 256]
[32m[1203 23:16:53 @registry.py:129][0m pool3 output: [None, 28, 28, 256]
[32m[1203 23:16:53 @registry.py:121][0m conv4_1 input: [None, 28, 28, 256]
[32m[1203 23:16:53 @registry.py:129][0m conv4_1 output: [None, 28, 28, 512]
[32m[1203 23:16:53 @registry.py:121][0m conv4_2 input: [None, 28, 28, 512]
[32m[1203 23:16:53 @registry.py:129][0m conv4_2 output: [None, 28, 28, 512]
[32m[1203 23:16:53 @registry.py:121][0m conv4_3 input: [None, 28, 28, 512]
[32m[1203 23:16:53 @registry.py:129][0m conv4_3 output: [None, 28, 28, 512]
[32m[1203 23:16:53 @registry.py:121][0m pool4 input: [None, 28, 28, 512]
[32m[1203 23:16:53 @registry.py:129][0m pool4 output: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:121][0m conv5_1 input: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:129][0m conv5_1 output: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:121][0m conv5_2 input: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:129][0m conv5_2 output: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:121][0m conv5_3 input: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:129][0m conv5_3 output: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:121][0m pool5 input: [None, 14, 14, 512]
[32m[1203 23:16:53 @registry.py:129][0m pool5 output: [None, 7, 7, 512]
[32m[1203 23:16:53 @registry.py:121][0m fc6_new input: [None, 7, 7, 512]
[32m[1203 23:16:53 @registry.py:129][0m fc6_new output: [None, 4096]
[32m[1203 23:16:53 @registry.py:121][0m fc7_new input: [None, 4096]
[32m[1203 23:16:53 @registry.py:129][0m fc7_new output: [None, 4096]
[32m[1203 23:16:53 @registry.py:121][0m fc8_new input: [None, 4096]
[32m[1203 23:16:53 @registry.py:129][0m fc8_new output: [None, 15]
[32m[1203 23:16:54 @develop.py:99][0m [5m[31mWRN[0m [Deprecated] get_cost() and self.cost will be deprecated after 30 Mar. Return the cost tensor directly in build_graph() instead!
[32m[1203 23:16:54 @develop.py:99][0m [5m[31mWRN[0m [Deprecated] get_scalar_var [/home/vigneshv/sfuhome/vigneshv/Computer_Vision_Ass_6/project6_package/code/run.py:238] will be deprecated after 01 Aug. Simply use tf.get_variable instead!
[32m[1203 23:16:54 @develop.py:99][0m [5m[31mWRN[0m [Deprecated] ModelDescBase._get_optimizer() interface will be deprecated after 30 Mar. Use optimizer() instead!
[32m[1203 23:16:54 @model_utils.py:64][0m [36mTrainable Variables: 
[0mname         shape                   dim
-----------  ----------------  ---------
conv1_1/W:0  [3, 3, 3, 64]          1728
conv1_1/b:0  [64]                     64
conv1_2/W:0  [3, 3, 64, 64]        36864
conv1_2/b:0  [64]                     64
conv2_1/W:0  [3, 3, 64, 128]       73728
conv2_1/b:0  [128]                   128
conv2_2/W:0  [3, 3, 128, 128]     147456
conv2_2/b:0  [128]                   128
conv3_1/W:0  [3, 3, 128, 256]     294912
conv3_1/b:0  [256]                   256
conv3_2/W:0  [3, 3, 256, 256]     589824
conv3_2/b:0  [256]                   256
conv3_3/W:0  [3, 3, 256, 256]     589824
conv3_3/b:0  [256]                   256
conv4_1/W:0  [3, 3, 256, 512]    1179648
conv4_1/b:0  [512]                   512
conv4_2/W:0  [3, 3, 512, 512]    2359296
conv4_2/b:0  [512]                   512
conv4_3/W:0  [3, 3, 512, 512]    2359296
conv4_3/b:0  [512]                   512
conv5_1/W:0  [3, 3, 512, 512]    2359296
conv5_1/b:0  [512]                   512
conv5_2/W:0  [3, 3, 512, 512]    2359296
conv5_2/b:0  [512]                   512
conv5_3/W:0  [3, 3, 512, 512]    2359296
conv5_3/b:0  [512]                   512
fc6_new/W:0  [25088, 4096]     102760448
fc6_new/b:0  [4096]                 4096
fc7_new/W:0  [4096, 4096]       16777216
fc7_new/b:0  [4096]                 4096
fc8_new/W:0  [4096, 15]            61440
fc8_new/b:0  [15]                     15[36m
Total #vars=32, #params=134321999, size=512.40MB[0m
[32m[1203 23:16:54 @base.py:209][0m Setup callbacks graph ...
[32m[1203 23:16:54 @inference_runner.py:154][0m [InferenceRunner] Building tower 'InferenceTower' on device /gpu:0 ...
[32m[1203 23:16:54 @develop.py:99][0m [5m[31mWRN[0m [Deprecated] ModelDescBase._build_graph() interface will be deprecated after 30 Mar. Use build_graph() instead!
[32m[1203 23:16:54 @summary.py:38][0m Maintain moving average summary of 2 tensors in collection MOVING_SUMMARY_OPS.
[32m[1203 23:16:54 @summary.py:75][0m Summarizing collection 'summaries' of size 19.
[32m[1203 23:16:54 @base.py:227][0m Creating the session ...
[32m[1203 23:17:01 @base.py:233][0m Initializing the session ...
[32m[1203 23:17:01 @sessinit.py:207][0m Variables to restore from dict: conv5_3/b:0, conv5_1/b:0, conv1_1/b:0, conv2_1/W:0, conv3_2/W:0, conv5_3/W:0, conv3_3/b:0, conv4_3/W:0, conv1_2/W:0, conv2_2/W:0, conv5_2/b:0, conv3_1/W:0, conv5_2/W:0, conv4_2/W:0, conv2_2/b:0, conv1_1/W:0, conv3_1/b:0, conv3_3/W:0, conv4_3/b:0, conv3_2/b:0, conv4_2/b:0, conv4_1/b:0, conv5_1/W:0, conv2_1/b:0, conv1_2/b:0, conv4_1/W:0
[32m[1203 23:17:01 @sessinit.py:90][0m [5m[31mWRN[0m The following variables are in the graph, but not found in the dict: fc6_new/W:0, fc6_new/b:0, fc7_new/W:0, fc7_new/b:0, fc8_new/W:0, fc8_new/b:0, global_step:0, learning_rate:0
[32m[1203 23:17:01 @sessinit.py:90][0m [5m[31mWRN[0m The following variables are in the dict, but not found in the graph: fc6/W:0, fc6/b:0, fc7/W:0, fc7/b:0, fc8/W:0, fc8/b:0
[32m[1203 23:17:01 @sessinit.py:220][0m Restoring from dict ...
[32m[1203 23:17:01 @base.py:240][0m Graph Finalized.
[32m[1203 23:17:01 @inference_runner.py:101][0m [InferenceRunner] Will eval 300 iterations
[32m[1203 23:17:01 @base.py:272][0m Start Epoch 1 ...
[32m[1203 23:51:44 @base.py:282][0m Epoch 1 (global_step 300) finished, time:34 minutes 43 seconds.
[32m[1203 23:53:18 @saver.py:77][0m Model saved to train_log/run/model-300.
[32m[1204 00:08:16 @monitor.py:459][0m cross_entropy_loss: 0.43113
[32m[1204 00:08:16 @monitor.py:459][0m learning_rate: 1e-05
[32m[1204 00:08:16 @monitor.py:459][0m train_error: 0.14635
[32m[1204 00:08:16 @monitor.py:459][0m validation_cost: 0.63675
[32m[1204 00:08:16 @monitor.py:459][0m validation_error: 0.20267
[32m[1204 00:08:16 @group.py:48][0m Callbacks took 991.176 sec in total. InferenceRunner: 14 minutes 57 seconds
[32m[1204 00:08:16 @base.py:272][0m Start Epoch 2 ...
[32m[1204 00:43:04 @base.py:282][0m Epoch 2 (global_step 600) finished, time:34 minutes 48 seconds.
[32m[1204 00:44:42 @saver.py:77][0m Model saved to train_log/run/model-600.
[32m[1204 00:59:42 @monitor.py:459][0m cross_entropy_loss: 0.21653
[32m[1204 00:59:42 @monitor.py:459][0m learning_rate: 1e-05
[32m[1204 00:59:42 @monitor.py:459][0m train_error: 0.072616
[32m[1204 00:59:42 @monitor.py:459][0m validation_cost: 0.40154
[32m[1204 00:59:42 @monitor.py:459][0m validation_error: 0.132
[32m[1204 00:59:42 @group.py:48][0m Callbacks took 997.579 sec in total. InferenceRunner: 14 minutes 59 seconds
[32m[1204 00:59:42 @base.py:286][0m Training has finished!
